{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3af1d1-54a3-4751-abad-7908bbdc0faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  name|salary|manager|\n+---+------+------+-------+\n| 10|  Anil| 50000|     18|\n| 11| Vikas| 75000|     16|\n| 12| Nisha| 40000|     18|\n| 13| Nidhi| 60000|     17|\n| 14| Priya| 80000|     18|\n| 15| Mohit| 45000|     18|\n| 16|Rajesh| 90000|     10|\n| 17| Raman| 55000|     16|\n| 18|   Sam| 65000|     17|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (10, \"Anil\", 50000, 18),\n",
    "    (11, \"Vikas\", 75000, 16),\n",
    "    (12, \"Nisha\", 40000, 18),\n",
    "    (13, \"Nidhi\", 60000, 17),\n",
    "    (14, \"Priya\", 80000, 18),\n",
    "    (15, \"Mohit\", 45000, 18),\n",
    "    (16, \"Rajesh\", 90000, 10),\n",
    "    (17, \"Raman\", 55000, 16),\n",
    "    (18, \"Sam\", 65000, 17),\n",
    "]\n",
    "\n",
    "schema = [\"id\", \"name\", \"salary\", \"manager\"]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c14990-91ef-4626-8ae7-36d93fcb8273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: 9"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc128e7e-33e2-4d5c-9cfa-2ac80840cbb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-------+\n| id| name|salary|manager|\n+---+-----+------+-------+\n| 19|Sohan| 50000|     18|\n| 20| Sima| 75000|     17|\n+---+-----+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(19, \"Sohan\", 50000, 18), (20, \"Sima\", 75000, 17)]\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238423e9-1e13-41dc-8378-f055cbc81df6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  name|salary|manager|\n+---+------+------+-------+\n| 10|  Anil| 50000|     18|\n| 11| Vikas| 75000|     16|\n| 12| Nisha| 40000|     18|\n| 13| Nidhi| 60000|     17|\n| 14| Priya| 80000|     18|\n| 15| Mohit| 45000|     18|\n| 16|Rajesh| 90000|     10|\n| 17| Raman| 55000|     16|\n| 18|   Sam| 65000|     17|\n| 19| Sohan| 50000|     18|\n| 20|  Sima| 75000|     17|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.union(df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c52f4a1-7bbd-4f28-bcf3-2cdbe4c3ddcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-------+\n| id| name|salary|manager|\n+---+-----+------+-------+\n| 19|Sohan| 50000|     18|\n| 20| Sima| 75000|     17|\n| 20| Sima| 75000|     17|\n+---+-----+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "duplicate_data = [\n",
    "    (19, \"Sohan\", 50000, 18),\n",
    "    (20, \"Sima\", 75000, 17),\n",
    "    (20, \"Sima\", 75000, 17),\n",
    "]\n",
    "\n",
    "duplicate_data_df = spark.createDataFrame(data=duplicate_data, schema=schema)\n",
    "duplicate_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba06b23-d558-4a26-b5a5-f5d6d1e53e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  name|salary|manager|\n+---+------+------+-------+\n| 10|  Anil| 50000|     18|\n| 11| Vikas| 75000|     16|\n| 12| Nisha| 40000|     18|\n| 13| Nidhi| 60000|     17|\n| 14| Priya| 80000|     18|\n| 15| Mohit| 45000|     18|\n| 16|Rajesh| 90000|     10|\n| 17| Raman| 55000|     16|\n| 18|   Sam| 65000|     17|\n| 19| Sohan| 50000|     18|\n| 20|  Sima| 75000|     17|\n| 20|  Sima| 75000|     17|\n+---+------+------+-------+\n\n+---+------+------+-------+\n| id|  name|salary|manager|\n+---+------+------+-------+\n| 10|  Anil| 50000|     18|\n| 11| Vikas| 75000|     16|\n| 12| Nisha| 40000|     18|\n| 13| Nidhi| 60000|     17|\n| 14| Priya| 80000|     18|\n| 15| Mohit| 45000|     18|\n| 16|Rajesh| 90000|     10|\n| 17| Raman| 55000|     16|\n| 18|   Sam| 65000|     17|\n| 19| Sohan| 50000|     18|\n| 20|  Sima| 75000|     17|\n| 20|  Sima| 75000|     17|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# In DataFrame union and unionAll works same\n",
    "df.unionAll(duplicate_data_df).show()\n",
    "df.union(duplicate_data_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2036425c-38c7-4f3d-af8a-e14dc20b8540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  name|salary|manager|\n+---+------+------+-------+\n| 10|  Anil| 50000|     18|\n| 11| Vikas| 75000|     16|\n| 12| Nisha| 40000|     18|\n| 13| Nidhi| 60000|     17|\n| 14| Priya| 80000|     18|\n| 15| Mohit| 45000|     18|\n| 16|Rajesh| 90000|     10|\n| 18|   Sam| 65000|     17|\n| 17| Raman| 55000|     16|\n| 19| Sohan| 50000|     18|\n| 20|  Sima| 75000|     17|\n+---+------+------+-------+\n\n+---+------+------+-------+\n| id|  name|salary|manager|\n+---+------+------+-------+\n| 10|  Anil| 50000|     18|\n| 11| Vikas| 75000|     16|\n| 12| Nisha| 40000|     18|\n| 13| Nidhi| 60000|     17|\n| 14| Priya| 80000|     18|\n| 15| Mohit| 45000|     18|\n| 16|Rajesh| 90000|     10|\n| 17| Raman| 55000|     16|\n| 18|   Sam| 65000|     17|\n| 19| Sohan| 50000|     18|\n| 20|  Sima| 75000|     17|\n| 20|  Sima| 75000|     17|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# But in Spark SQL union all do not remove duplicate entries\n",
    "# we have to convert DataFrame in to a table\n",
    "df.createOrReplaceTempView(\"customers\")\n",
    "duplicate_data_df.createOrReplaceTempView(\"customers2\")\n",
    "query_union = \"\"\"\n",
    "    select * from customers\n",
    "    union\n",
    "    select * from customers2\n",
    "    \"\"\"\n",
    "\n",
    "query_union_all = \"\"\"\n",
    "    select * from customers\n",
    "    union all\n",
    "    select * from customers2\n",
    "    \"\"\"\n",
    "spark.sql(query_union).show()\n",
    "spark.sql(query_union_all).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a0b1df-1253-4c5f-9862-98309efad10a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+-----+\n| id|salary|manager| name|\n+---+------+-------+-----+\n| 19| 50000|     18|Sohan|\n| 20| 75000|     17| Sima|\n+---+------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "wrong_order_data = [(19, 50000, 18, \"Sohan\"), (20, 75000, 17, \"Sima\")]\n",
    "wrong_schema = [\"id\", \"salary\", \"manager\", \"name\"]\n",
    "wrong_order_df = spark.createDataFrame(data=wrong_order_data, schema=wrong_schema)\n",
    "wrong_order_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc4e018-f6a6-4ff9-ab27-ba53397f6ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  name|salary|manager|\n+---+------+------+-------+\n| 10|  Anil| 50000|     18|\n| 11| Vikas| 75000|     16|\n| 12| Nisha| 40000|     18|\n| 13| Nidhi| 60000|     17|\n| 14| Priya| 80000|     18|\n| 15| Mohit| 45000|     18|\n| 16|Rajesh| 90000|     10|\n| 17| Raman| 55000|     16|\n| 18|   Sam| 65000|     17|\n| 19| Sohan| 50000|     18|\n| 20|  Sima| 75000|     17|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# if we want to union wrong order DataFrames we have to use UnionByName\n",
    "df.unionByName(wrong_order_df).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Union , Union all & UnionByName",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}